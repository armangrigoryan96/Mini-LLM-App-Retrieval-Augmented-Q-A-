"""
Evaluation Script for RAG System

Evaluates the RAG system using:
1. Recall@k - measures retrieval quality
2. Answer similarity - measures answer quality using embeddings
"""

import json
import time
import os
from typing import List, Dict, Tuple
from collections import defaultdict

import numpy as np
from openai import OpenAI

from src.vector_store import VectorStore
from src.rag_pipeline import RAGPipeline
from src.qa_dataset import qa_dataset


class RAGEvaluator:
    """Evaluator for RAG system performance."""
    
    def __init__(self, vector_store: VectorStore, rag_pipeline: RAGPipeline):
        """Initialize the evaluator.
        
        Args:
            vector_store: Vector store for retrieval evaluation
            rag_pipeline: RAG pipeline for answer generation
        """
        self.vector_store = vector_store
        self.rag_pipeline = rag_pipeline
        
        # Use OpenAI for embeddings
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.embedding_model_name = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
        
        print("RAG Evaluator initialized")
    
    def calculate_recall_at_k(
        self,
        question: str,
        relevant_doc_ids: List[str],
        k: int = 5
    ) -> float:
        """Calculate Recall@k for a single question.
        
        Args:
            question: The question to evaluate
            relevant_doc_ids: List of relevant document IDs
            k: Number of top results to consider
            
        Returns:
            Recall@k score (0 to 1)
        """
        # Retrieve top-k documents
        results = self.vector_store.retrieve(question, top_k=k)
        
        # Extract source URLs from results
        retrieved_sources = set()
        for result in results:
            source_url = result['metadata']['source']
            # Extract document ID from URL (e.g., "sql-create-index" from URL)
            doc_id = source_url.split('/')[-1].replace('.html', '')
            retrieved_sources.add(doc_id)
        
        # Calculate recall
        relevant_set = set(relevant_doc_ids)
        retrieved_relevant = retrieved_sources.intersection(relevant_set)
        
        if len(relevant_set) == 0:
            return 1.0  # No relevant docs means perfect recall by default
        
        recall = len(retrieved_relevant) / len(relevant_set)
        return recall
    
    def calculate_answer_similarity(
        self,
        generated_answer: str,
        reference_answer: str
    ) -> float:
        """Calculate cosine similarity between generated and reference answers.
        
        Args:
            generated_answer: Answer generated by the RAG system
            reference_answer: Ground truth reference answer
            
        Returns:
            Cosine similarity score (0 to 1)
        """
        # Generate embeddings using OpenAI
        response = self.openai_client.embeddings.create(
            input=[generated_answer, reference_answer],
            model=self.embedding_model_name
        )
        
        emb1 = np.array(response.data[0].embedding)
        emb2 = np.array(response.data[1].embedding)
        
        # Calculate cosine similarity manually
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        
        return float(similarity)
    
    def evaluate_single_question(
        self,
        qa_item: Dict,
        k: int = 5
    ) -> Dict:
        """Evaluate a single question from the dataset.
        
        Args:
            qa_item: Dictionary with question, reference_answer, and relevant_docs
            k: Number of top results for Recall@k
            
        Returns:
            Dictionary with evaluation metrics
        """
        question = qa_item['question']
        reference_answer = qa_item['reference_answer']
        relevant_docs = qa_item['relevant_docs']
        
        # Calculate Recall@k
        recall = self.calculate_recall_at_k(question, relevant_docs, k=k)
        
        # Generate answer
        response = self.rag_pipeline.answer_question(question, check_relevance=False)
        generated_answer = response['answer']
        
        # Calculate answer similarity
        similarity = self.calculate_answer_similarity(generated_answer, reference_answer)
        
        return {
            'question_id': qa_item['id'],
            'question': question,
            'reference_answer': reference_answer,
            'generated_answer': generated_answer,
            'recall_at_k': recall,
            'answer_similarity': similarity,
            'sources_retrieved': len(response['sources']),
            'category': qa_item.get('category', 'Unknown')
        }
    
    def evaluate_dataset(
        self,
        dataset: List[Dict],
        k: int = 5,
        save_results: bool = True
    ) -> Dict:
        """Evaluate the entire dataset.
        
        Args:
            dataset: List of QA items
            k: Number of top results for Recall@k
            save_results: Whether to save detailed results to file
            
        Returns:
            Dictionary with aggregate metrics and detailed results
        """
        print(f"\nEvaluating {len(dataset)} questions...")
        print("="*80)
        
        results = []
        category_metrics = defaultdict(lambda: {'recall': [], 'similarity': []})
        
        for i, qa_item in enumerate(dataset, 1):
            print(f"\nEvaluating question {i}/{len(dataset)}: {qa_item['question'][:60]}...")
            
            result = self.evaluate_single_question(qa_item, k=k)
            results.append(result)
            
            # Aggregate by category
            category = result['category']
            category_metrics[category]['recall'].append(result['recall_at_k'])
            category_metrics[category]['similarity'].append(result['answer_similarity'])
            
            print(f"  Recall@{k}: {result['recall_at_k']:.3f}")
            print(f"  Answer Similarity: {result['answer_similarity']:.3f}")
            
            # Rate limiting to avoid API issues
            time.sleep(0.5)
        
        # Calculate aggregate metrics
        all_recalls = [r['recall_at_k'] for r in results]
        all_similarities = [r['answer_similarity'] for r in results]
        
        aggregate_metrics = {
            'total_questions': len(dataset),
            'k': k,
            'avg_recall_at_k': np.mean(all_recalls),
            'std_recall_at_k': np.std(all_recalls),
            'min_recall_at_k': np.min(all_recalls),
            'max_recall_at_k': np.max(all_recalls),
            'avg_answer_similarity': np.mean(all_similarities),
            'std_answer_similarity': np.std(all_similarities),
            'min_answer_similarity': np.min(all_similarities),
            'max_answer_similarity': np.max(all_similarities),
        }
        
        # Category-wise metrics
        category_stats = {}
        for category, metrics in category_metrics.items():
            category_stats[category] = {
                'count': len(metrics['recall']),
                'avg_recall': np.mean(metrics['recall']),
                'avg_similarity': np.mean(metrics['similarity'])
            }
        
        evaluation_results = {
            'aggregate_metrics': aggregate_metrics,
            'category_metrics': category_stats,
            'detailed_results': results
        }
        
        # Save results
        if save_results:
            output_file = 'evaluation_results.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
            print(f"\nResults saved to {output_file}")
        
        return evaluation_results
    
    def print_summary(self, evaluation_results: Dict):
        """Print a summary of evaluation results.
        
        Args:
            evaluation_results: Results from evaluate_dataset
        """
        metrics = evaluation_results['aggregate_metrics']
        
        print("\n" + "="*80)
        print("EVALUATION SUMMARY")
        print("="*80)
        
        print(f"\nTotal Questions: {metrics['total_questions']}")
        print(f"k value: {metrics['k']}")
        
        print(f"\n{'Metric':<30} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}")
        print("-"*80)
        print(f"{'Recall@k':<30} {metrics['avg_recall_at_k']:<10.3f} "
              f"{metrics['std_recall_at_k']:<10.3f} {metrics['min_recall_at_k']:<10.3f} "
              f"{metrics['max_recall_at_k']:<10.3f}")
        print(f"{'Answer Similarity':<30} {metrics['avg_answer_similarity']:<10.3f} "
              f"{metrics['std_answer_similarity']:<10.3f} {metrics['min_answer_similarity']:<10.3f} "
              f"{metrics['max_answer_similarity']:<10.3f}")
        
        # Category breakdown
        print(f"\n{'Category Breakdown'}")
        print("-"*80)
        category_metrics = evaluation_results['category_metrics']
        
        print(f"{'Category':<20} {'Count':<10} {'Avg Recall':<15} {'Avg Similarity':<15}")
        print("-"*80)
        for category, stats in sorted(category_metrics.items()):
            print(f"{category:<20} {stats['count']:<10} {stats['avg_recall']:<15.3f} "
                  f"{stats['avg_similarity']:<15.3f}")
        
        print("\n" + "="*80)


def main():
    """Main evaluation function."""
    print("Starting RAG System Evaluation")
    print("="*80)
    
    # Initialize components
    print("\nInitializing vector store...")
    vector_store = VectorStore()
    
    print("Initializing RAG pipeline...")
    rag_pipeline = RAGPipeline(vector_store, temperature=0.1, top_k=5)
    
    # Initialize evaluator
    evaluator = RAGEvaluator(vector_store, rag_pipeline)
    
    # Run evaluation
    results = evaluator.evaluate_dataset(qa_dataset, k=5, save_results=True)
    
    # Print summary
    evaluator.print_summary(results)
    
    # Print some example results
    print("\n" + "="*80)
    print("EXAMPLE RESULTS (First 3 questions)")
    print("="*80)
    
    for i, result in enumerate(results['detailed_results'][:3], 1):
        print(f"\n--- Question {i} ---")
        print(f"Q: {result['question']}")
        print(f"\nReference: {result['reference_answer'][:200]}...")
        print(f"\nGenerated: {result['generated_answer'][:200]}...")
        print(f"\nMetrics:")
        print(f"  Recall@5: {result['recall_at_k']:.3f}")
        print(f"  Similarity: {result['answer_similarity']:.3f}")


if __name__ == "__main__":
    main()
